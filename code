import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score

# Load the dataset
df = pd.read_csv('wineqt.csv')

# Display first 5 rows
print("First 5 rows of the dataset:")
print(df.head())

# Display last 5 rows
print("\nLast 5 rows of the dataset:")
print(df.tail())

# Dataset shape
print("\nDataset shape (rows, columns):")
print(df.shape)

# Information about the dataset
print("\nDataset Info:")
df.info()

# Check for missing values
print("\nMissing values per column:")
print(df.isnull().sum())

# Remove duplicates
duplicate_count = df.duplicated().sum()
print(f"\nNumber of duplicate rows: {duplicate_count}")

df.drop_duplicates(inplace=True)
print(f"Data shape after removing duplicates: {df.shape}")

# Features and target
# Check if 'Quality' is present and correctly named
if 'Quality' not in df.columns:
    raise ValueError("'Quality' column not found in the dataset. Please check the column names.")

X = df.drop('Quality', axis=1)
y = df['Quality']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize RandomForestClassifier with class balancing
clf = RandomForestClassifier(class_weight='balanced', random_state=42)

# Train the classifier
clf.fit(X_train, y_train)


# Predictions on the test data
y_pred = clf.predict(X_test)

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, zero_division=1))

# Confusion matrix
print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# Confusion matrix visualization
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Histograms of features
df.hist(bins=10, figsize=(12,10))
plt.show()

# Feature importance
feature_importances = clf.feature_importances_
features = X.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Print Feature Importances
print("\nFeature Importances:")
print(importance_df)

# Plot feature importances 
plt.figure(figsize=(10,6))
sns.barplot(x='Feature', y='Importance', data=importance_df, palette='viridis')
plt.title('Feature Importances')
plt.xticks(rotation=90)  # Rotate x-axis labels for readability
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.tight_layout()  # Adjust layout to prevent label clipping
plt.show()

# Cross-validation on the training data
scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')
print(f"\nCross-Validation Scores: {scores}")
print(f"Mean Cross-Validation Score: {scores.mean()}")
